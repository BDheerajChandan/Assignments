{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5074f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\License Plate Recognition.docx\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\license_plates_detection_train\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\license_plates_recognition_train\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\Licplatesdetection_train.csv\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\Licplatesrecognition_train.csv\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\Main.ipynb\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\SampleSubmission.csv\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\test\n",
      "c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\~$cense Plate Recognition.docx\n",
      "Index(['img_id', 'ymin', 'xmin', 'ymax', 'xmax'], dtype='object')\n",
      "Index(['img_id', 'text'], dtype='object')\n",
      "Index(['id', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='object')\n",
      "1\n",
      "900\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "print(os.getcwd())\n",
    "for i in os.listdir():\n",
    "    print(str(os.path.join(os.getcwd(),i)))\n",
    "\n",
    "for i in os.listdir():\n",
    "    if str(i).endswith(\".csv\"):\n",
    "        print(pd.read_csv(i).columns)\n",
    "\n",
    "l=[r\"c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\test\",\n",
    "r\"c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\license_plates_detection_train\",\n",
    "r\"c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\license_plates_recognition_train\"]\n",
    "for i in l:\n",
    "    print(len(os.listdir(path=i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3b08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading and preprocessing recognition images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:01<00:00, 865.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Encoding plate text labels...\n",
      "ðŸ§  Building CNN model...\n",
      "ðŸŽ¯ Training CNN model...\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 9s 271ms/step - loss: 6.4006 - accuracy: 0.0012 - val_loss: 6.3974 - val_accuracy: 0.0111\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 6.3878 - accuracy: 0.0074 - val_loss: 6.4103 - val_accuracy: 0.0111\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 6s 250ms/step - loss: 6.3674 - accuracy: 0.0074 - val_loss: 6.4872 - val_accuracy: 0.0111\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 7s 268ms/step - loss: 6.3220 - accuracy: 0.0049 - val_loss: 6.5950 - val_accuracy: 0.0111\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 7s 262ms/step - loss: 6.2394 - accuracy: 0.0148 - val_loss: 6.7902 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 6.1343 - accuracy: 0.0247 - val_loss: 7.0913 - val_accuracy: 0.0111\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 5.9357 - accuracy: 0.0321 - val_loss: 6.9089 - val_accuracy: 0.0222\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 5.6290 - accuracy: 0.0481 - val_loss: 8.0022 - val_accuracy: 0.0111\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 5.1692 - accuracy: 0.0901 - val_loss: 7.7104 - val_accuracy: 0.0333\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 4.6723 - accuracy: 0.1469 - val_loss: 8.5536 - val_accuracy: 0.0444\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 7s 256ms/step - loss: 4.0999 - accuracy: 0.2049 - val_loss: 9.5803 - val_accuracy: 0.0333\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 3.4199 - accuracy: 0.2914 - val_loss: 9.9838 - val_accuracy: 0.0444\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 2.8418 - accuracy: 0.3901 - val_loss: 10.5889 - val_accuracy: 0.0556\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 7s 268ms/step - loss: 2.4796 - accuracy: 0.4407 - val_loss: 11.5914 - val_accuracy: 0.0667\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 7s 258ms/step - loss: 2.0916 - accuracy: 0.5037 - val_loss: 11.8726 - val_accuracy: 0.0444\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 7s 256ms/step - loss: 1.8469 - accuracy: 0.5741 - val_loss: 12.1889 - val_accuracy: 0.0444\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 7s 252ms/step - loss: 1.6230 - accuracy: 0.6173 - val_loss: 13.3619 - val_accuracy: 0.0667\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.3395 - accuracy: 0.6568 - val_loss: 13.9275 - val_accuracy: 0.0778\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.3287 - accuracy: 0.6889 - val_loss: 14.8332 - val_accuracy: 0.0667\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.1307 - accuracy: 0.7185 - val_loss: 14.1820 - val_accuracy: 0.0778\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.0204 - accuracy: 0.7296 - val_loss: 15.2442 - val_accuracy: 0.0889\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.9551 - accuracy: 0.7420 - val_loss: 14.9923 - val_accuracy: 0.1000\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.8465 - accuracy: 0.7728 - val_loss: 16.3128 - val_accuracy: 0.1000\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.8334 - accuracy: 0.7852 - val_loss: 16.7818 - val_accuracy: 0.0889\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.8546 - accuracy: 0.7741 - val_loss: 16.7386 - val_accuracy: 0.0889\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.7614 - accuracy: 0.7988 - val_loss: 16.5063 - val_accuracy: 0.0889\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.9295 - accuracy: 0.7617 - val_loss: 16.7455 - val_accuracy: 0.0778\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.6809 - accuracy: 0.8346 - val_loss: 16.8463 - val_accuracy: 0.0778\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.6983 - accuracy: 0.8074 - val_loss: 16.5635 - val_accuracy: 0.0889\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.7018 - accuracy: 0.8012 - val_loss: 18.2188 - val_accuracy: 0.0889\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.6231 - accuracy: 0.8420 - val_loss: 18.2087 - val_accuracy: 0.1111\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.6092 - accuracy: 0.8346 - val_loss: 17.7007 - val_accuracy: 0.1000\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.6461 - accuracy: 0.8407 - val_loss: 17.5757 - val_accuracy: 0.0556\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.5647 - accuracy: 0.8481 - val_loss: 19.2187 - val_accuracy: 0.0667\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.6561 - accuracy: 0.8148 - val_loss: 17.7421 - val_accuracy: 0.0889\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.5356 - accuracy: 0.8469 - val_loss: 18.6301 - val_accuracy: 0.0889\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.5907 - accuracy: 0.8457 - val_loss: 18.8795 - val_accuracy: 0.0667\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.6185 - accuracy: 0.8407 - val_loss: 16.6002 - val_accuracy: 0.0889\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.5993 - accuracy: 0.8383 - val_loss: 18.0413 - val_accuracy: 0.0889\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 0.6289 - accuracy: 0.8235 - val_loss: 16.6012 - val_accuracy: 0.1000\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.5761 - accuracy: 0.8457 - val_loss: 18.7816 - val_accuracy: 0.0778\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.6150 - accuracy: 0.8272 - val_loss: 18.2074 - val_accuracy: 0.0889\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.4821 - accuracy: 0.8556 - val_loss: 19.2263 - val_accuracy: 0.0889\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4260 - accuracy: 0.8815 - val_loss: 19.6995 - val_accuracy: 0.1000\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.4395 - accuracy: 0.8728 - val_loss: 19.7758 - val_accuracy: 0.0778\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.4747 - accuracy: 0.8667 - val_loss: 21.1616 - val_accuracy: 0.0778\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4595 - accuracy: 0.8580 - val_loss: 19.6520 - val_accuracy: 0.0667\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.4801 - accuracy: 0.8679 - val_loss: 19.2080 - val_accuracy: 0.0778\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 0.4456 - accuracy: 0.8802 - val_loss: 21.0306 - val_accuracy: 0.0667\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 0.4585 - accuracy: 0.8630 - val_loss: 20.6519 - val_accuracy: 0.0778\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.4421 - accuracy: 0.8778 - val_loss: 20.4762 - val_accuracy: 0.0889\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4455 - accuracy: 0.8704 - val_loss: 20.4913 - val_accuracy: 0.1000\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.4076 - accuracy: 0.8864 - val_loss: 20.3995 - val_accuracy: 0.0778\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.4280 - accuracy: 0.8938 - val_loss: 20.1582 - val_accuracy: 0.0889\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4688 - accuracy: 0.8556 - val_loss: 19.6405 - val_accuracy: 0.0889\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.4327 - accuracy: 0.8716 - val_loss: 19.0574 - val_accuracy: 0.0889\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3858 - accuracy: 0.9000 - val_loss: 19.8860 - val_accuracy: 0.1000\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.4137 - accuracy: 0.8827 - val_loss: 20.3552 - val_accuracy: 0.1111\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.4176 - accuracy: 0.8926 - val_loss: 20.6509 - val_accuracy: 0.0889\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.4262 - accuracy: 0.8852 - val_loss: 19.7147 - val_accuracy: 0.0778\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3999 - accuracy: 0.8790 - val_loss: 20.8717 - val_accuracy: 0.1000\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3227 - accuracy: 0.9025 - val_loss: 20.3089 - val_accuracy: 0.1000\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.3746 - accuracy: 0.8877 - val_loss: 20.0565 - val_accuracy: 0.1111\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.3695 - accuracy: 0.9000 - val_loss: 20.7617 - val_accuracy: 0.0778\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4253 - accuracy: 0.8827 - val_loss: 19.9746 - val_accuracy: 0.0889\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.3475 - accuracy: 0.9012 - val_loss: 20.8876 - val_accuracy: 0.1111\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.4027 - accuracy: 0.8852 - val_loss: 19.0387 - val_accuracy: 0.1000\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.4251 - accuracy: 0.8691 - val_loss: 19.4075 - val_accuracy: 0.1222\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.4025 - accuracy: 0.8840 - val_loss: 20.6397 - val_accuracy: 0.1000\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.3660 - accuracy: 0.8901 - val_loss: 20.2947 - val_accuracy: 0.0889\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.3491 - accuracy: 0.8975 - val_loss: 19.7122 - val_accuracy: 0.0889\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 6s 250ms/step - loss: 0.3736 - accuracy: 0.8864 - val_loss: 20.4772 - val_accuracy: 0.1111\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 0.4128 - accuracy: 0.8877 - val_loss: 19.9868 - val_accuracy: 0.1000\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.3647 - accuracy: 0.9037 - val_loss: 21.2949 - val_accuracy: 0.1000\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3317 - accuracy: 0.9062 - val_loss: 21.4637 - val_accuracy: 0.0889\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3410 - accuracy: 0.8864 - val_loss: 20.9298 - val_accuracy: 0.1000\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3471 - accuracy: 0.8975 - val_loss: 21.0116 - val_accuracy: 0.1000\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.2726 - accuracy: 0.9222 - val_loss: 22.7131 - val_accuracy: 0.0889\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.2879 - accuracy: 0.9173 - val_loss: 22.2967 - val_accuracy: 0.1000\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.3090 - accuracy: 0.9160 - val_loss: 21.3110 - val_accuracy: 0.1000\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.2682 - accuracy: 0.9235 - val_loss: 22.1415 - val_accuracy: 0.0889\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3599 - accuracy: 0.8889 - val_loss: 22.5272 - val_accuracy: 0.1000\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3234 - accuracy: 0.9074 - val_loss: 21.0142 - val_accuracy: 0.1000\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.4045 - accuracy: 0.8877 - val_loss: 20.5197 - val_accuracy: 0.1000\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.3655 - accuracy: 0.8901 - val_loss: 21.0512 - val_accuracy: 0.0889\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.2671 - accuracy: 0.9123 - val_loss: 21.8959 - val_accuracy: 0.1000\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.3251 - accuracy: 0.8988 - val_loss: 22.2540 - val_accuracy: 0.1111\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.3183 - accuracy: 0.9049 - val_loss: 22.2088 - val_accuracy: 0.0778\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.3481 - accuracy: 0.9012 - val_loss: 21.9536 - val_accuracy: 0.1000\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.3490 - accuracy: 0.9012 - val_loss: 22.4686 - val_accuracy: 0.1000\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.2923 - accuracy: 0.9111 - val_loss: 21.8045 - val_accuracy: 0.1000\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.2561 - accuracy: 0.9210 - val_loss: 23.0402 - val_accuracy: 0.1111\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.3557 - accuracy: 0.8938 - val_loss: 20.9160 - val_accuracy: 0.1000\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 0.2917 - accuracy: 0.9123 - val_loss: 20.9996 - val_accuracy: 0.0889\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.3313 - accuracy: 0.9123 - val_loss: 22.0285 - val_accuracy: 0.1000\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.2988 - accuracy: 0.9000 - val_loss: 23.4537 - val_accuracy: 0.1111\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 7s 267ms/step - loss: 0.2804 - accuracy: 0.9148 - val_loss: 23.3416 - val_accuracy: 0.1111\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.2772 - accuracy: 0.9185 - val_loss: 20.5111 - val_accuracy: 0.1111\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.3182 - accuracy: 0.9074 - val_loss: 24.2091 - val_accuracy: 0.1000\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.3691 - accuracy: 0.8988 - val_loss: 20.1901 - val_accuracy: 0.0889\n",
      "âœ… Model training complete and saved to:\n",
      "  - c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\lp_recognition_model.h5\n",
      "  - c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\label_encoder.pkl\n",
      "\n",
      "ðŸ§ª Running on test set...\n",
      "ðŸ” Detecting and recognizing license plates on test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:17<00:00, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Submission saved to: c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\SampleSubmission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "# === PATH SETUP ===\n",
    "base_path = r\"c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\"\n",
    "detect_img_dir = os.path.join(base_path, \"license_plates_detection_train\")\n",
    "recog_img_dir = os.path.join(base_path, \"license_plates_recognition_train\")\n",
    "test_img_dir = os.path.join(base_path, \"test\")\n",
    "\n",
    "detect_csv_path = os.path.join(base_path, \"Licplatesdetection_train.csv\")\n",
    "recog_csv_path = os.path.join(base_path, \"Licplatesrecognition_train.csv\")\n",
    "\n",
    "# New output directory for saving models and submission\n",
    "output_dir = os.path.join(base_path, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "submission_path = os.path.join(output_dir, \"SampleSubmission.csv\")\n",
    "\n",
    "# === TRAINING RECOGNITION MODEL ===\n",
    "df_recog = pd.read_csv(recog_csv_path)\n",
    "\n",
    "X, y = [], []\n",
    "print(\"ðŸ“¦ Loading and preprocessing recognition images...\")\n",
    "for _, row in tqdm(df_recog.iterrows(), total=len(df_recog), desc=\"Loading images\"):\n",
    "    img_path = os.path.join(recog_img_dir, row['img_id'])\n",
    "    if os.path.exists(img_path):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 64))  # Width=128, Height=64\n",
    "        X.append(img)\n",
    "        y.append(row['text'])\n",
    "    else:\n",
    "        print(f\"âš ï¸ Image not found: {img_path}\")\n",
    "\n",
    "X = np.array(X).reshape(-1, 64, 128, 1) / 255.0\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels (whole plate text classification)\n",
    "print(\"ðŸ”¤ Encoding plate text labels...\")\n",
    "label_encoder = LabelBinarizer()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "print(\"ðŸ§  Building CNN model...\")\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64, 128, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with 100 epochs\n",
    "print(\"ðŸŽ¯ Training CNN model...\")\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)\n",
    "\n",
    "# Save model and label encoder to new output folder\n",
    "model_save_path = os.path.join(output_dir, \"lp_recognition_model.h5\")\n",
    "label_encoder_path = os.path.join(output_dir, \"label_encoder.pkl\")\n",
    "\n",
    "model.save(model_save_path)\n",
    "with open(label_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"âœ… Model training complete and saved to:\\n  - {model_save_path}\\n  - {label_encoder_path}\")\n",
    "\n",
    "# === TESTING & PREDICTION ===\n",
    "print(\"\\nðŸ§ª Running on test set...\")\n",
    "test_img_files = sorted(os.listdir(test_img_dir))\n",
    "predictions = []\n",
    "\n",
    "# Load model and encoder from new output folder\n",
    "model = load_model(model_save_path)\n",
    "with open(label_encoder_path, \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(\"ðŸ” Detecting and recognizing license plates on test images...\")\n",
    "for img_file in tqdm(test_img_files, desc=\"Test images\"):\n",
    "    img_path = os.path.join(test_img_dir, img_file)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        # If image loading failed\n",
    "        predictions.append([img_file] + [\"\"] * 10)  # Pad 10 columns for submission\n",
    "        continue\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    cx, cy = w // 2, h // 2\n",
    "    dw, dh = int(w * 0.3), int(h * 0.15)  # 60% crop width and 30% crop height approx.\n",
    "    xmin, xmax = max(0, cx - dw), min(w, cx + dw)\n",
    "    ymin, ymax = max(0, cy - dh), min(h, cy + dh)\n",
    "    plate_crop = img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Save cropped plate image to output folder as JPG\n",
    "    plate_img_save_path = os.path.join(output_dir, f\"{os.path.splitext(img_file)[0]}_plate.jpg\")\n",
    "    cv2.imwrite(plate_img_save_path, plate_crop)\n",
    "\n",
    "    try:\n",
    "        plate_crop_gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n",
    "        plate_crop_gray = cv2.resize(plate_crop_gray, (128, 64))\n",
    "        plate_crop_gray = plate_crop_gray.reshape(1, 64, 128, 1) / 255.0\n",
    "\n",
    "        # Suppress keras verbose output here for clean progress bar\n",
    "        pred = model.predict(plate_crop_gray, verbose=0)\n",
    "        pred_label = label_encoder.inverse_transform(pred)[0]\n",
    "\n",
    "        # Pad with space ' ' which is exactly one character\n",
    "        padded = list(pred_label.ljust(10, ' '))\n",
    "        predictions.append([img_file] + padded[:10])\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"âš ï¸ Prediction error on {img_file}: {e}\")\n",
    "        predictions.append([img_file] + [\"\"] * 10)\n",
    "\n",
    "# Save submission CSV in new output folder\n",
    "df_sub = pd.DataFrame(predictions, columns=['id'] + [str(i) for i in range(10)])\n",
    "df_sub.to_csv(submission_path, index=False)\n",
    "print(f\"ðŸ“„ Submission saved to: {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69123396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading and preprocessing recognition images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:00<00:00, 965.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Encoding plate text labels...\n",
      "ðŸ§  Building CNN model...\n",
      "ðŸŽ¯ Training CNN model...\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 242ms/step - loss: 6.4109 - accuracy: 0.0037 - val_loss: 6.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 7s 263ms/step - loss: 6.3767 - accuracy: 0.0099 - val_loss: 6.4259 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 7s 285ms/step - loss: 6.3337 - accuracy: 0.0123 - val_loss: 6.7218 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 6.2462 - accuracy: 0.0160 - val_loss: 6.5622 - val_accuracy: 0.0111\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 7s 252ms/step - loss: 6.1036 - accuracy: 0.0173 - val_loss: 7.0177 - val_accuracy: 0.0111\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 5.8742 - accuracy: 0.0370 - val_loss: 6.8849 - val_accuracy: 0.0111\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 7s 250ms/step - loss: 5.5718 - accuracy: 0.0605 - val_loss: 7.6404 - val_accuracy: 0.0111\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 5.0252 - accuracy: 0.0963 - val_loss: 8.3790 - val_accuracy: 0.0222\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 4.5078 - accuracy: 0.1469 - val_loss: 8.6194 - val_accuracy: 0.0222\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 3.7676 - accuracy: 0.2494 - val_loss: 9.4619 - val_accuracy: 0.0333\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 7s 252ms/step - loss: 3.2029 - accuracy: 0.3469 - val_loss: 10.6987 - val_accuracy: 0.0444\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 2.4270 - accuracy: 0.4642 - val_loss: 10.2468 - val_accuracy: 0.0556\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 2.1878 - accuracy: 0.4975 - val_loss: 11.9514 - val_accuracy: 0.0556\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.7589 - accuracy: 0.5728 - val_loss: 12.3595 - val_accuracy: 0.0444\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.4925 - accuracy: 0.6407 - val_loss: 12.7959 - val_accuracy: 0.0667\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.3996 - accuracy: 0.6481 - val_loss: 12.8330 - val_accuracy: 0.0667\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.1160 - accuracy: 0.7284 - val_loss: 13.9446 - val_accuracy: 0.0444\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.9770 - accuracy: 0.7580 - val_loss: 15.2146 - val_accuracy: 0.0667\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.9410 - accuracy: 0.7667 - val_loss: 14.2142 - val_accuracy: 0.0778\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.8974 - accuracy: 0.7679 - val_loss: 14.3134 - val_accuracy: 0.0667\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.8804 - accuracy: 0.7642 - val_loss: 14.6455 - val_accuracy: 0.0778\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 6s 250ms/step - loss: 0.7923 - accuracy: 0.7815 - val_loss: 15.6284 - val_accuracy: 0.0778\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 0.7745 - accuracy: 0.7926 - val_loss: 15.6131 - val_accuracy: 0.0667\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 7s 261ms/step - loss: 0.7616 - accuracy: 0.7988 - val_loss: 16.2592 - val_accuracy: 0.0556\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.6761 - accuracy: 0.8247 - val_loss: 17.0324 - val_accuracy: 0.0889\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.6198 - accuracy: 0.8346 - val_loss: 16.6821 - val_accuracy: 0.0667\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 6s 237ms/step - loss: 0.5524 - accuracy: 0.8531 - val_loss: 17.2561 - val_accuracy: 0.0556\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.5439 - accuracy: 0.8346 - val_loss: 16.7400 - val_accuracy: 0.0778\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 6s 236ms/step - loss: 0.4690 - accuracy: 0.8642 - val_loss: 17.0884 - val_accuracy: 0.0778\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.5329 - accuracy: 0.8667 - val_loss: 17.2315 - val_accuracy: 0.0889\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.4863 - accuracy: 0.8543 - val_loss: 17.5041 - val_accuracy: 0.0889\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 0.5508 - accuracy: 0.8617 - val_loss: 16.6056 - val_accuracy: 0.0778\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.4253 - accuracy: 0.8691 - val_loss: 19.2512 - val_accuracy: 0.0778\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 0.4816 - accuracy: 0.8654 - val_loss: 18.5790 - val_accuracy: 0.0889\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.4730 - accuracy: 0.8679 - val_loss: 17.1513 - val_accuracy: 0.0889\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.4381 - accuracy: 0.8741 - val_loss: 19.2866 - val_accuracy: 0.0889\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 0.4490 - accuracy: 0.8852 - val_loss: 19.1222 - val_accuracy: 0.1000\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.4282 - accuracy: 0.8840 - val_loss: 18.0240 - val_accuracy: 0.1000\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.4096 - accuracy: 0.8827 - val_loss: 19.1685 - val_accuracy: 0.0778\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 0.4312 - accuracy: 0.8790 - val_loss: 18.7481 - val_accuracy: 0.1000\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.3779 - accuracy: 0.8951 - val_loss: 19.3917 - val_accuracy: 0.0778\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.3932 - accuracy: 0.9037 - val_loss: 18.3176 - val_accuracy: 0.0889\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.3723 - accuracy: 0.8951 - val_loss: 19.7390 - val_accuracy: 0.0778\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.3101 - accuracy: 0.9160 - val_loss: 19.5911 - val_accuracy: 0.0778\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.3092 - accuracy: 0.9099 - val_loss: 20.5610 - val_accuracy: 0.0667\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.3809 - accuracy: 0.8951 - val_loss: 18.4215 - val_accuracy: 0.0778\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.3341 - accuracy: 0.9025 - val_loss: 19.6588 - val_accuracy: 0.0778\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.3392 - accuracy: 0.8926 - val_loss: 20.4311 - val_accuracy: 0.1000\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.4277 - accuracy: 0.8778 - val_loss: 17.4664 - val_accuracy: 0.1000\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.3508 - accuracy: 0.8914 - val_loss: 19.4378 - val_accuracy: 0.1000\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.3084 - accuracy: 0.9062 - val_loss: 19.5148 - val_accuracy: 0.0778\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.2616 - accuracy: 0.9321 - val_loss: 20.7673 - val_accuracy: 0.1000\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.2868 - accuracy: 0.9173 - val_loss: 20.1830 - val_accuracy: 0.0667\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2876 - accuracy: 0.9198 - val_loss: 21.0998 - val_accuracy: 0.0778\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 7s 266ms/step - loss: 0.3423 - accuracy: 0.9111 - val_loss: 20.5781 - val_accuracy: 0.0778\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.4030 - accuracy: 0.9012 - val_loss: 19.5571 - val_accuracy: 0.0889\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.3489 - accuracy: 0.8951 - val_loss: 19.8115 - val_accuracy: 0.0778\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.3559 - accuracy: 0.8988 - val_loss: 20.2215 - val_accuracy: 0.0667\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 0.3416 - accuracy: 0.9074 - val_loss: 19.9015 - val_accuracy: 0.0889\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3452 - accuracy: 0.9074 - val_loss: 19.4910 - val_accuracy: 0.0889\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3337 - accuracy: 0.9049 - val_loss: 19.3078 - val_accuracy: 0.1000\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 6s 250ms/step - loss: 0.2993 - accuracy: 0.9136 - val_loss: 20.6748 - val_accuracy: 0.0667\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3016 - accuracy: 0.9099 - val_loss: 22.9302 - val_accuracy: 0.0778\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.2589 - accuracy: 0.9185 - val_loss: 19.9092 - val_accuracy: 0.1000\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.2803 - accuracy: 0.9160 - val_loss: 20.9356 - val_accuracy: 0.0889\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2413 - accuracy: 0.9284 - val_loss: 21.1685 - val_accuracy: 0.0889\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.3355 - accuracy: 0.9074 - val_loss: 20.2990 - val_accuracy: 0.1000\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 0.3149 - accuracy: 0.9173 - val_loss: 19.0792 - val_accuracy: 0.0778\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 7s 252ms/step - loss: 0.3097 - accuracy: 0.9012 - val_loss: 20.7207 - val_accuracy: 0.0889\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 7s 264ms/step - loss: 0.2853 - accuracy: 0.9210 - val_loss: 20.9178 - val_accuracy: 0.1000\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 7s 268ms/step - loss: 0.2488 - accuracy: 0.9210 - val_loss: 19.9625 - val_accuracy: 0.0667\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 0.2751 - accuracy: 0.9222 - val_loss: 19.9207 - val_accuracy: 0.0778\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.2951 - accuracy: 0.9123 - val_loss: 20.0670 - val_accuracy: 0.1000\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.3100 - accuracy: 0.9123 - val_loss: 19.6611 - val_accuracy: 0.1000\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.3134 - accuracy: 0.9123 - val_loss: 20.0487 - val_accuracy: 0.0778\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 0.3141 - accuracy: 0.9062 - val_loss: 19.4489 - val_accuracy: 0.1000\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.2126 - accuracy: 0.9370 - val_loss: 22.4929 - val_accuracy: 0.0889\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.2552 - accuracy: 0.9309 - val_loss: 21.9962 - val_accuracy: 0.0778\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2497 - accuracy: 0.9284 - val_loss: 20.7906 - val_accuracy: 0.0778\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2675 - accuracy: 0.9272 - val_loss: 21.5237 - val_accuracy: 0.0889\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.2560 - accuracy: 0.9210 - val_loss: 20.8390 - val_accuracy: 0.0667\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.2016 - accuracy: 0.9420 - val_loss: 20.8685 - val_accuracy: 0.0667\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2041 - accuracy: 0.9395 - val_loss: 22.5779 - val_accuracy: 0.1000\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 0.2151 - accuracy: 0.9494 - val_loss: 23.6059 - val_accuracy: 0.0667\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.2324 - accuracy: 0.9346 - val_loss: 20.1716 - val_accuracy: 0.0778\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 0.3209 - accuracy: 0.9210 - val_loss: 20.4226 - val_accuracy: 0.0556\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 0.2498 - accuracy: 0.9321 - val_loss: 21.2220 - val_accuracy: 0.0556\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.2091 - accuracy: 0.9383 - val_loss: 20.9950 - val_accuracy: 0.0667\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 0.1970 - accuracy: 0.9395 - val_loss: 22.0135 - val_accuracy: 0.0667\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 0.2773 - accuracy: 0.9160 - val_loss: 20.0642 - val_accuracy: 0.0778\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 0.2256 - accuracy: 0.9358 - val_loss: 20.0755 - val_accuracy: 0.0667\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 0.2221 - accuracy: 0.9296 - val_loss: 20.5549 - val_accuracy: 0.0667\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 0.1299 - accuracy: 0.9605 - val_loss: 24.2910 - val_accuracy: 0.0667\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 6s 249ms/step - loss: 0.2913 - accuracy: 0.9160 - val_loss: 19.9084 - val_accuracy: 0.0556\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 0.2091 - accuracy: 0.9494 - val_loss: 19.7563 - val_accuracy: 0.0778\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 0.2403 - accuracy: 0.9272 - val_loss: 20.7729 - val_accuracy: 0.0778\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 0.2237 - accuracy: 0.9346 - val_loss: 22.2261 - val_accuracy: 0.0556\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 0.1860 - accuracy: 0.9444 - val_loss: 22.1306 - val_accuracy: 0.0556\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.2292 - accuracy: 0.9346 - val_loss: 21.2193 - val_accuracy: 0.0556\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 0.1962 - accuracy: 0.9407 - val_loss: 22.4359 - val_accuracy: 0.0556\n",
      "âœ… Model training complete and saved to:\n",
      "  - c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\lp_recognition_model.h5\n",
      "  - c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\label_encoder.pkl\n",
      "\n",
      "ðŸ§ª Running on test set...\n",
      "ðŸ” Detecting and recognizing license plates on test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:17<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Submission saved to: c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\\output\\SampleSubmission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === IMPORT LIBRARIES ===\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "# === PATH SETUP ===\n",
    "# Base directory for all input/output files\n",
    "base_path = r\"c:\\Users\\KIIT\\Desktop\\GitHUB\\Assignments\\soulpageit\"\n",
    "\n",
    "# Training data directories\n",
    "detect_img_dir = os.path.join(base_path, \"license_plates_detection_train\")\n",
    "recog_img_dir = os.path.join(base_path, \"license_plates_recognition_train\")\n",
    "test_img_dir = os.path.join(base_path, \"test\")\n",
    "\n",
    "# CSV annotation files\n",
    "detect_csv_path = os.path.join(base_path, \"Licplatesdetection_train.csv\")\n",
    "recog_csv_path = os.path.join(base_path, \"Licplatesrecognition_train.csv\")\n",
    "\n",
    "# Output directory for saving model, encoder, and submission\n",
    "output_dir = os.path.join(base_path, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create if not exists\n",
    "\n",
    "# Output submission file path\n",
    "submission_path = os.path.join(output_dir, \"SampleSubmission.csv\")\n",
    "\n",
    "# === STEP 1: LOAD AND PREPROCESS RECOGNITION DATA ===\n",
    "df_recog = pd.read_csv(recog_csv_path)\n",
    "\n",
    "X, y = [], []\n",
    "print(\"ðŸ“¦ Loading and preprocessing recognition images...\")\n",
    "for _, row in tqdm(df_recog.iterrows(), total=len(df_recog), desc=\"Loading images\"):\n",
    "    img_path = os.path.join(recog_img_dir, row['img_id'])\n",
    "    if os.path.exists(img_path):\n",
    "        # Load image in grayscale\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 64))  # Resize to fixed dimensions\n",
    "        X.append(img)\n",
    "        y.append(row['text'])  # Plate text\n",
    "    else:\n",
    "        print(f\"âš ï¸ Image not found: {img_path}\")\n",
    "\n",
    "# Convert to numpy array and normalize\n",
    "X = np.array(X).reshape(-1, 64, 128, 1) / 255.0  # Normalize to [0,1]\n",
    "y = np.array(y)\n",
    "\n",
    "# === STEP 2: ENCODE TEXT LABELS ===\n",
    "print(\"ðŸ”¤ Encoding plate text labels...\")\n",
    "label_encoder = LabelBinarizer()\n",
    "y_encoded = label_encoder.fit_transform(y)  # One-hot encoding for plate strings\n",
    "\n",
    "# === STEP 3: SPLIT TRAIN/VALIDATION DATA ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "# === STEP 4: DEFINE CNN MODEL ===\n",
    "print(\"ðŸ§  Building CNN model...\")\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64, 128, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevent overfitting\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === STEP 5: TRAIN MODEL ===\n",
    "print(\"ðŸŽ¯ Training CNN model...\")\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)\n",
    "\n",
    "# === STEP 6: SAVE MODEL AND LABEL ENCODER ===\n",
    "model_save_path = os.path.join(output_dir, \"lp_recognition_model.h5\")\n",
    "label_encoder_path = os.path.join(output_dir, \"label_encoder.pkl\")\n",
    "\n",
    "model.save(model_save_path)\n",
    "with open(label_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"âœ… Model training complete and saved to:\\n  - {model_save_path}\\n  - {label_encoder_path}\")\n",
    "\n",
    "# === STEP 7: RUN ON TEST SET ===\n",
    "print(\"\\nðŸ§ª Running on test set...\")\n",
    "test_img_files = sorted(os.listdir(test_img_dir))\n",
    "predictions = []\n",
    "\n",
    "# Load saved model and encoder\n",
    "model = load_model(model_save_path)\n",
    "with open(label_encoder_path, \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(\"ðŸ” Detecting and recognizing license plates on test images...\")\n",
    "for img_file in tqdm(test_img_files, desc=\"Test images\"):\n",
    "    img_path = os.path.join(test_img_dir, img_file)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        # Could not load image; fill with blanks\n",
    "        predictions.append([img_file] + [\"\"] * 10)\n",
    "        continue\n",
    "\n",
    "    # Simple plate localization: center crop\n",
    "    h, w = img.shape[:2]\n",
    "    cx, cy = w // 2, h // 2\n",
    "    dw, dh = int(w * 0.3), int(h * 0.15)  # Approx crop box\n",
    "    xmin, xmax = max(0, cx - dw), min(w, cx + dw)\n",
    "    ymin, ymax = max(0, cy - dh), min(h, cy + dh)\n",
    "    plate_crop = img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Save cropped plate for visual inspection\n",
    "    plate_img_save_path = os.path.join(output_dir, f\"{os.path.splitext(img_file)[0]}_plate.jpg\")\n",
    "    cv2.imwrite(plate_img_save_path, plate_crop)\n",
    "\n",
    "    try:\n",
    "        # Preprocess cropped plate image\n",
    "        plate_crop_gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n",
    "        plate_crop_gray = cv2.resize(plate_crop_gray, (128, 64))\n",
    "        plate_crop_gray = plate_crop_gray.reshape(1, 64, 128, 1) / 255.0\n",
    "\n",
    "        # Predict plate text\n",
    "        pred = model.predict(plate_crop_gray, verbose=0)\n",
    "        pred_label = label_encoder.inverse_transform(pred)[0]\n",
    "\n",
    "        # Pad prediction to 10 characters\n",
    "        padded = list(pred_label.ljust(10, ' '))\n",
    "        predictions.append([img_file] + padded[:10])\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"âš ï¸ Prediction error on {img_file}: {e}\")\n",
    "        predictions.append([img_file] + [\"\"] * 10)\n",
    "\n",
    "# === STEP 8: SAVE SUBMISSION FILE ===\n",
    "df_sub = pd.DataFrame(predictions, columns=['id'] + [str(i) for i in range(10)])\n",
    "df_sub.to_csv(submission_path, index=False)\n",
    "print(f\"ðŸ“„ Submission saved to: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
